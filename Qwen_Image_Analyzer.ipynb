{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgT75hzeB6kT"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q gradio\n",
        "!pip install -q gTTS\n",
        "!pip install -q -U transformers==4.37.2\n",
        "!pip install -q bitsandbytes==0.41.3 accelerate==0.25.0\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Chat With Image"
      ],
      "metadata": {
        "id": "q6NwfmdxCKgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from gtts import gTTS\n",
        "import torch\n",
        "from PIL import Image\n",
        "import speech_recognition as sr\n",
        "\n",
        "# Load model and processor\n",
        "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_name, torch_dtype=torch.bfloat16, attn_implementation=\"eager\", device_map=\"auto\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Function to process audio to text\n",
        "def transcribe_audio(audio_path):\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.AudioFile(audio_path) as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "    try:\n",
        "        return recognizer.recognize_google(audio_data)\n",
        "    except sr.UnknownValueError:\n",
        "        return \"Could not understand the audio.\"\n",
        "    except sr.RequestError as e:\n",
        "        return f\"Speech recognition error: {e}\"\n",
        "\n",
        "# Function to process image and query\n",
        "def analyze_image(image_path, text_query, audio_query_path):\n",
        "    # Handle voice input if provided\n",
        "    if audio_query_path:\n",
        "        processed_query = transcribe_audio(audio_query_path)\n",
        "    else:\n",
        "        processed_query = text_query\n",
        "\n",
        "    if not image_path or not processed_query.strip():\n",
        "        return \"Please provide both an image and a query.\", None\n",
        "\n",
        "    # Load and process the image\n",
        "    pil_image = Image.open(image_path)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"image\": \"<insert_image_path_or_url_here>\",\n",
        "                    \"max_pixels\": 360 * 420,\n",
        "                    \"fps\": 1.0,\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": processed_query,\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "    text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=[pil_image],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
        "\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
        "    ]\n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "    )[0]\n",
        "\n",
        "    # Convert text to speech\n",
        "    tts = gTTS(output_text, lang=\"en\")\n",
        "    audio_file = \"output_audio.mp3\"\n",
        "    tts.save(audio_file)\n",
        "\n",
        "    return output_text, audio_file\n",
        "\n",
        "# Define Gradio interface\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"## Image Analyzer with Voice Assistant\")\n",
        "\n",
        "    with gr.Row():\n",
        "        image_input = gr.Image(type=\"filepath\", label=\"Upload Image\")\n",
        "        query_input = gr.Textbox(label=\"Type Your Query\", placeholder=\"Enter your query here...\")\n",
        "        voice_input = gr.Audio(type=\"filepath\", label=\"Or Record Your Query\")\n",
        "\n",
        "    analyze_button = gr.Button(\"Analyze\")\n",
        "\n",
        "    with gr.Row():\n",
        "        output_text = gr.Textbox(label=\"Analysis Result\", interactive=False)\n",
        "        output_audio = gr.Audio(label=\"Audio Response\", interactive=False)\n",
        "\n",
        "    analyze_button.click(\n",
        "        fn=analyze_image,\n",
        "        inputs=[image_input, query_input, voice_input],\n",
        "        outputs=[output_text, output_audio]\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "app.launch()\n"
      ],
      "metadata": {
        "id": "-VbggmfVB7PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNlkXz2-B7Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cRaKQ31SB7Yp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}